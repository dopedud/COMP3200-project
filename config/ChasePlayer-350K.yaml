behaviors:
    ChasePlayer:
        trainer_type: ppo

        # Trainer configurations common to all trainers
        max_steps: 350000
        time_horizon: 64
        summary_freq: 5000
        keep_checkpoints: 5
        checkpoint_interval: 100000
        threaded: false
        init_path: null

        hyperparameters:

            # Hyperparameters common to PPO and SAC
            batch_size: 1024
            buffer_size: 10240
            learning_rate: 3.0e-4
            learning_rate_schedule: linear

            # PPO-specific hyperparameters
            beta: 5.0e-3
            beta_schedule: constant
            epsilon: 0.2
            epsilon_schedule: linear
            lambd: 0.95
            num_epoch: 3
            shared_critic: false

        # Configuration of the neural network (common to PPO/SAC)
        network_settings:
            vis_encode_type: simple
            conditioning_type: hyper
            normalize: false
            hidden_units: 128
            num_layers: 4

        reward_signals:

            # environment reward (default)
            extrinsic:
                strength: 1.0
                gamma: 0.99

            # curiosity module
            curiosity:
                strength: 0.1
                gamma: 0.8