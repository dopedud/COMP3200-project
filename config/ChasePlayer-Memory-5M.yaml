behaviors:
    ChasePlayer:
        trainer_type: ppo

        # Trainer configurations common to all trainers
        max_steps: 5e6
        time_horizon: 64
        summary_freq: 5000
        keep_checkpoints: 5
        checkpoint_interval: 100000
        threaded: false
        init_path: null

        hyperparameters:

            # Hyperparameters common to PPO and SAC
            batch_size: 1024
            buffer_size: 10240
            learning_rate: 3.0e-4
            learning_rate_schedule: constant

            # PPO-specific hyperparameters
            beta: 5.0e-3
            #beta_schedule: learning_rate_schedule
            epsilon: 0.2
            #epsilon_schedule: learning_rate_schedule
            lambd: 0.95
            num_epoch: 3
            shared_critic: false

        # Configuration of the neural network (common to PPO/SAC)
        network_settings:
            vis_encode_type: simple
            conditioning_type: hyper
            normalize: false
            hidden_units: 384
            num_layers: 4
            memory:
                memory_size: 128
                sequence_length: 64

        reward_signals:

            # extrinsic reward
            extrinsic:
                strength: 1.0
                gamma: 0.99

            # curiosity intrinsic reward
            curiosity:
                strength: 0.1
                gamma: 0.99